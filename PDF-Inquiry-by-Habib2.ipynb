{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e445e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Local PDF Chat with Gradio ‚Äì 100 % offline\n",
    "Fixed for LangChain ‚â• 0.3  &  Gradio ‚â• 4.44\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from difflib import get_close_matches\n",
    "\n",
    "import gradio as gr\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --------------------------------------------------\n",
    "# CONSTANTS\n",
    "# --------------------------------------------------\n",
    "MODEL_NAME = \"llama3\"\n",
    "ROOT = \"vectorstores\"\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# HELPERS\n",
    "# --------------------------------------------------\n",
    "def build_vectorstore(pdf_path: str) -> str:\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    vs_path = os.path.join(ROOT, pdf_name)\n",
    "\n",
    "    if os.path.isdir(vs_path) and os.listdir(vs_path):\n",
    "        return vs_path  # already ingested\n",
    "\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    splits = splitter.split_documents(docs)\n",
    "\n",
    "    embeddings = OllamaEmbeddings(model=MODEL_NAME)\n",
    "    Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=vs_path,\n",
    "    )\n",
    "    return vs_path\n",
    "\n",
    "\n",
    "def load_qa_chain(vs_path: str) -> RetrievalQA:\n",
    "    embeddings = OllamaEmbeddings(model=MODEL_NAME)\n",
    "    vectorstore = Chroma(\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=vs_path,\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 8})\n",
    "    llm = OllamaLLM(model=MODEL_NAME)\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=False,\n",
    "        chain_type=\"stuff\",\n",
    "    )\n",
    "\n",
    "# --------------------------------------------------\n",
    "# SESSION STATE\n",
    "# --------------------------------------------------\n",
    "class State:\n",
    "    pdf_path: str = \"\"\n",
    "    vocab: list = []\n",
    "    qa_chain = None\n",
    "\n",
    "\n",
    "state = State()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# GRADIO CALLBACKS\n",
    "# --------------------------------------------------\n",
    "def upload_pdf(pdf_file):\n",
    "    if pdf_file is None:\n",
    "        return \"No file uploaded.\", gr.update(visible=False)\n",
    "\n",
    "    state.pdf_path = pdf_file\n",
    "    vs_path = build_vectorstore(pdf_file)\n",
    "    state.qa_chain = load_qa_chain(vs_path)\n",
    "\n",
    "    # build vocab for fuzzy correction\n",
    "    raw = \" \".join([d.page_content for d in PyPDFLoader(pdf_file).load()]).lower()\n",
    "    state.vocab = list(set(re.findall(r\"\\b\\w+\\b\", raw)))\n",
    "\n",
    "    return f\"‚úÖ PDF ingested ‚Üí vectorstore at `{vs_path}`.\", gr.update(visible=True)\n",
    "\n",
    "\n",
    "def chat(history, question):\n",
    "    if state.qa_chain is None:\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"Please upload a PDF first.\"})\n",
    "        return history, \"\"\n",
    "\n",
    "    # fuzzy spell correction\n",
    "    words = question.split()\n",
    "    fixed = [get_close_matches(w.lower(), state.vocab, n=1, cutoff=0.8)[0]\n",
    "             if get_close_matches(w.lower(), state.vocab, n=1, cutoff=0.8)\n",
    "             else w for w in words]\n",
    "    corrected = \" \".join(fixed)\n",
    "\n",
    "    if corrected != question:\n",
    "        history.append({\"role\": \"user\", \"content\": question})\n",
    "        history.append({\"role\": \"assistant\", \"content\": f\"‚úèÔ∏è Corrected to: {corrected}\"})\n",
    "        question = corrected\n",
    "\n",
    "    answer = state.qa_chain.invoke(question)[\"result\"].strip()\n",
    "    if not answer or \"no answer\" in answer.lower() or \"i don't know\" in answer.lower():\n",
    "        answer = \"üîç No.\"\n",
    "    history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "    return history, \"\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# GRADIO UI\n",
    "# --------------------------------------------------\n",
    "with gr.Blocks(title=\"Local PDF Chat\") as app:\n",
    "    gr.Markdown(\"# üìÑ PDF Inquiry by Habib (Ollama ‚Äì 100 % offline)\")\n",
    "\n",
    "    with gr.Tab(\"About\"):\n",
    "        gr.Markdown(\"\"\"\n",
    " \n",
    "                    \n",
    "        ## ü§ñ Local PDF Chat  \n",
    "        A completely **offline** Gradio application that lets you\n",
    "        upload any PDF, index it locally with **Chroma + Ollama**, and\n",
    "        then ask questions that are answered **only** from the document.\n",
    "\n",
    "        ### üîë Key Features\n",
    "        - No external API keys  \n",
    "        - On-device embedding & LLM (Ollama)  \n",
    "        - Fuzzy spell-correction based on PDF vocabulary  \n",
    "\n",
    "        ## üë§ About the Author\n",
    "        **Dr. Habib Ullah Manzoor**  \n",
    "        Ph.D. Trustworthy Distributed Computing, University of Glasgow  \n",
    "        10+ yrs ML, federated learning, cybersecurity  \n",
    "        [LinkedIn](https://www.linkedin.com/in/habib-ullah-manzoor-phd-19198994/) | [Google Scholar](https://scholar.google.com.pk/citations?user=tKDhmdAAAAAJ&hl=en) | [ORCID](https://orcid.org/0000-0003-0192-7353)  \n",
    "        üìß habibullahmanzoor@gmail.com\n",
    "        \"\"\")\n",
    "\n",
    "    with gr.Tab(\"Upload PDF\"):\n",
    "        pdf_input = gr.File(label=\"Choose PDF\", file_types=[\".pdf\"])\n",
    "        status = gr.Textbox(label=\"Status\", interactive=False)\n",
    "        upload_btn = gr.Button(\"Process PDF\")\n",
    "\n",
    "    with gr.Tab(\"Chat\") as chat_tab:\n",
    "        chatbot = gr.Chatbot(label=\"Conversation\", height=400, type=\"messages\")\n",
    "        msg = gr.Textbox(label=\"Your question\", placeholder=\"Ask anything about the uploaded PDF‚Ä¶\")\n",
    "        clear = gr.Button(\"Clear\")\n",
    "\n",
    "        msg.submit(chat, [chatbot, msg], [chatbot, msg])\n",
    "        clear.click(lambda: [], None, chatbot, queue=False)\n",
    "\n",
    "    upload_btn.click(upload_pdf, inputs=pdf_input, outputs=[status, chat_tab])\n",
    "    chat_tab.visible = False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
